---
title: "Topic models lab key"
author: "Jeff Leek"
date: "October 9, 2016"
output: html_document
---

1. Download the Simply Statistics Github repo from here: https://github.com/simplystats/simplystats.github.io

2. Read in the text files from the `_posts` subdirectory of the resulting set of files. You will want to use the `tm` package for this. The functions you will need are `DirSource` and `Vcorpus`. 

```{r}
library(tm)
ds = DirSource("_posts/")
simply = VCorpus(ds)
```

3. Now look at the meta data for the 926th document using the `meta` command 

```{r}
meta(simply[[926]])
```

4. Now use the `tidy` command to tidy up the documents and then unnest the tokens. 

```{r}
library(dplyr)
library(tidytext)
tidy_simply = simply %>% tidy %>%
  unnest_tokens(word,text) %>% select(author,datetimestamp,id,word)
```

5. Remove the stopwords

```{r}
data("stop_words")
tidy_simply = tidy_simply %>% anti_join(stop_words)
```

6. Calculate the most frequent words using `group_by`, `count`, and `arrange`

```{r}
most_freq = tidy_simply %>% group_by(word) %>% 
  count() %>% arrange(desc(n))
most_freq
```

7. Only keep words in this list of English words: https://github.com/dwyl/english-words/blob/master/words.txt.zip and remove the 20 most frequent words. 


```{r}
library(readr)
eng_words = read_csv("../words.txt",col_names=FALSE)
names(eng_words) = "word"
tidy_simply = tidy_simply %>% inner_join(eng_words)
most_freq = tidy_simply %>% group_by(word) %>% 
  count() %>% arrange(desc(n))
tidy_simply = tidy_simply %>% anti_join(most_freq[1:20,])
```

8. Cast the tidy obect into a DocumentTermMatrix object. 

```{r}
simply_dtm = cast_dtm_(tidy_simply,document="id",term="word",value=1)
```

9. Use the `LDA` command in the `topicmodels` package to fit a topic model using 3 and 10 topics. 

```{r}
lda3 = tidy(LDA(simply_dtm,3))
lda10 = tidy(LDA(simply_dtm,10))
```

10. Make a wordcloud of the top 20 words from each of these models. Can you "label" any of them. 

```{r}
libary(wordcloud)
word_func = function(ldaobj,top,n){
  obj = ldaobj %>% filter(topic==top) %>%
    arrange(desc(beta))
  obj = obj[1:n,]
  wordcloud(obj$term,freq=obj$beta)
}
```



